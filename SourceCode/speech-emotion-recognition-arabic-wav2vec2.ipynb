{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\npaths = []\nlabels = []\ncounter = 0\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if counter < 4:\n            counter += 1\n            continue\n        paths.append(os.path.join(dirname, filename))\n        label = filename.split('-')[4]\n        labels.append(label)\n\nprint('Dataset is Loaded')","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:46:20.731209Z","iopub.execute_input":"2023-03-12T20:46:20.731603Z","iopub.status.idle":"2023-03-12T20:46:21.680692Z","shell.execute_reply.started":"2023-03-12T20:46:20.731569Z","shell.execute_reply":"2023-03-12T20:46:21.679489Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Dataset is Loaded\n","output_type":"stream"}]},{"cell_type":"code","source":"paths[:5]","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:46:21.683288Z","iopub.execute_input":"2023-03-12T20:46:21.684079Z","iopub.status.idle":"2023-03-12T20:46:21.691964Z","shell.execute_reply.started":"2023-03-12T20:46:21.684039Z","shell.execute_reply":"2023-03-12T20:46:21.690878Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"['/kaggle/input/basic-arabic-vocal-emotions-dataset/remake/remake/2/51-m-20-2-2-767.wav',\n '/kaggle/input/basic-arabic-vocal-emotions-dataset/remake/remake/2/56-f-40-2-0-627.wav',\n '/kaggle/input/basic-arabic-vocal-emotions-dataset/remake/remake/2/56-f-40-2-1-639.wav',\n '/kaggle/input/basic-arabic-vocal-emotions-dataset/remake/remake/2/55-m-16-2-2-809.wav',\n '/kaggle/input/basic-arabic-vocal-emotions-dataset/remake/remake/2/4-m-20-2-0-699.wav']"},"metadata":{}}]},{"cell_type":"code","source":"labels[:5]","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:46:21.693618Z","iopub.execute_input":"2023-03-12T20:46:21.694553Z","iopub.status.idle":"2023-03-12T20:46:21.710157Z","shell.execute_reply.started":"2023-03-12T20:46:21.694497Z","shell.execute_reply":"2023-03-12T20:46:21.708875Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['2', '0', '1', '2', '0']"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n## Create a dataframe\ndf = pd.DataFrame()\ndf['speech'] = paths\ndf['label'] = labels\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:46:21.713380Z","iopub.execute_input":"2023-03-12T20:46:21.714177Z","iopub.status.idle":"2023-03-12T20:46:21.747392Z","shell.execute_reply.started":"2023-03-12T20:46:21.714137Z","shell.execute_reply":"2023-03-12T20:46:21.745908Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                              speech label\n0  /kaggle/input/basic-arabic-vocal-emotions-data...     2\n1  /kaggle/input/basic-arabic-vocal-emotions-data...     0\n2  /kaggle/input/basic-arabic-vocal-emotions-data...     1\n3  /kaggle/input/basic-arabic-vocal-emotions-data...     2\n4  /kaggle/input/basic-arabic-vocal-emotions-data...     0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>speech</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/basic-arabic-vocal-emotions-data...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/basic-arabic-vocal-emotions-data...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/basic-arabic-vocal-emotions-data...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/basic-arabic-vocal-emotions-data...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/basic-arabic-vocal-emotions-data...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"label2id = {'tired/exhausted': 0, 'neutral': 1, 'positive/negative': 2}\nid2label = {0: 'tired/exhausted', 1: 'neutral', 2: 'positive/negative'}\n\n# Replace 'label' column with integer values\ndf['label'] = df['label'].replace({'0': 0, '1': 1, '2': 2})\n# Create 'emotion' column based on 'labels' column\ndf['emotion'] = df['label'].map(id2label)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:46:21.749102Z","iopub.execute_input":"2023-03-12T20:46:21.749878Z","iopub.status.idle":"2023-03-12T20:46:21.768918Z","shell.execute_reply.started":"2023-03-12T20:46:21.749840Z","shell.execute_reply":"2023-03-12T20:46:21.767836Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                              speech  label            emotion\n0  /kaggle/input/basic-arabic-vocal-emotions-data...      2  positive/negative\n1  /kaggle/input/basic-arabic-vocal-emotions-data...      0    tired/exhausted\n2  /kaggle/input/basic-arabic-vocal-emotions-data...      1            neutral\n3  /kaggle/input/basic-arabic-vocal-emotions-data...      2  positive/negative\n4  /kaggle/input/basic-arabic-vocal-emotions-data...      0    tired/exhausted","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>speech</th>\n      <th>label</th>\n      <th>emotion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/basic-arabic-vocal-emotions-data...</td>\n      <td>2</td>\n      <td>positive/negative</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/basic-arabic-vocal-emotions-data...</td>\n      <td>0</td>\n      <td>tired/exhausted</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/basic-arabic-vocal-emotions-data...</td>\n      <td>1</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/basic-arabic-vocal-emotions-data...</td>\n      <td>2</td>\n      <td>positive/negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/basic-arabic-vocal-emotions-data...</td>\n      <td>0</td>\n      <td>tired/exhausted</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(df[\"label\"].dtype)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:46:21.770395Z","iopub.execute_input":"2023-03-12T20:46:21.771025Z","iopub.status.idle":"2023-03-12T20:46:21.776144Z","shell.execute_reply.started":"2023-03-12T20:46:21.770990Z","shell.execute_reply":"2023-03-12T20:46:21.775090Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"int64\n","output_type":"stream"}]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:46:21.777581Z","iopub.execute_input":"2023-03-12T20:46:21.778190Z","iopub.status.idle":"2023-03-12T20:46:21.790551Z","shell.execute_reply.started":"2023-03-12T20:46:21.778155Z","shell.execute_reply":"2023-03-12T20:46:21.789408Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(3870, 3)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# We need to specify the input and output column\ninput_column = \"speech\"\noutput_column = \"label\"\n\ntrain_dataframe, eval_dataframe = train_test_split(df, test_size=0.2, random_state=101, stratify=df[\"label\"])\n\ntrain_dataframe = train_dataframe.reset_index(drop=True)\neval_dataframe = eval_dataframe.reset_index(drop=True)\n\nfrom datasets import Dataset\n\n# Convert the dataframe to a Dataset object\ntrain_dataset = Dataset.from_pandas(train_dataframe)\neval_dataset = Dataset.from_pandas(eval_dataframe)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:46:21.792694Z","iopub.execute_input":"2023-03-12T20:46:21.793597Z","iopub.status.idle":"2023-03-12T20:46:24.989058Z","shell.execute_reply.started":"2023-03-12T20:46:21.793560Z","shell.execute_reply":"2023-03-12T20:46:24.988046Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(train_dataset)\nprint(eval_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:46:24.990329Z","iopub.execute_input":"2023-03-12T20:46:24.990916Z","iopub.status.idle":"2023-03-12T20:46:25.001223Z","shell.execute_reply.started":"2023-03-12T20:46:24.990877Z","shell.execute_reply":"2023-03-12T20:46:25.000161Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['speech', 'label', 'emotion'],\n    num_rows: 3096\n})\nDataset({\n    features: ['speech', 'label', 'emotion'],\n    num_rows: 774\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"wav2vec2 Model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoConfig, Wav2Vec2Processor","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:46:25.006987Z","iopub.execute_input":"2023-03-12T20:46:25.007280Z","iopub.status.idle":"2023-03-12T20:46:38.273847Z","shell.execute_reply.started":"2023-03-12T20:46:25.007252Z","shell.execute_reply":"2023-03-12T20:46:38.272741Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:46:38.275249Z","iopub.execute_input":"2023-03-12T20:46:38.276160Z","iopub.status.idle":"2023-03-12T20:46:38.282768Z","shell.execute_reply.started":"2023-03-12T20:46:38.276126Z","shell.execute_reply":"2023-03-12T20:46:38.281711Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model_name_or_path = \"elgeish/wav2vec2-large-xlsr-53-arabic\"\npooling_mode = \"mean\"\nnum_labels = 3","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:46:38.284695Z","iopub.execute_input":"2023-03-12T20:46:38.285411Z","iopub.status.idle":"2023-03-12T20:46:38.307243Z","shell.execute_reply.started":"2023-03-12T20:46:38.285374Z","shell.execute_reply":"2023-03-12T20:46:38.306127Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# config\nconfig = AutoConfig.from_pretrained(\n    model_name_or_path,\n    num_labels=num_labels,\n    label2id=label2id,\n    id2label=id2label,\n    finetuning_task=\"wav2vec2_clf\",\n)\nsetattr(config, 'pooling_mode', pooling_mode)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:46:38.310078Z","iopub.execute_input":"2023-03-12T20:46:38.310404Z","iopub.status.idle":"2023-03-12T20:46:38.662894Z","shell.execute_reply.started":"2023-03-12T20:46:38.310376Z","shell.execute_reply":"2023-03-12T20:46:38.661824Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28f55d31dc4a40029d1e1bcead16c0a7"}},"metadata":{}}]},{"cell_type":"code","source":"processor = Wav2Vec2Processor.from_pretrained(model_name_or_path,)\ntarget_sampling_rate = processor.feature_extractor.sampling_rate\nprint(f\"The target sampling rate: {target_sampling_rate}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:46:38.664568Z","iopub.execute_input":"2023-03-12T20:46:38.665257Z","iopub.status.idle":"2023-03-12T20:46:40.078505Z","shell.execute_reply.started":"2023-03-12T20:46:38.665215Z","shell.execute_reply":"2023-03-12T20:46:40.077400Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)rocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c19f827457ea4b1f8b960961fec0af43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/303 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"219b4485c92d4053b49b4e705ec000e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbd13bd398724168a32a5e4a977c9ae0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"437f5a2448d34a49b7be3377dc4648d6"}},"metadata":{}},{"name":"stdout","text":"The target sampling rate: 16000\n","output_type":"stream"}]},{"cell_type":"code","source":"import librosa\n# Define the speech_file_to_array_fn function\ndef speech_file_to_array_fn(path):\n    signal, sr = librosa.load(path, sr=target_sampling_rate)\n    return signal\n\n# Define the preprocess_function function\ndef preprocess_function(examples):\n    speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]\n    target_list = [label for label in examples[output_column]]\n    result = processor(speech_list, sampling_rate=target_sampling_rate, return_attention_mask=True)\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:46:40.080029Z","iopub.execute_input":"2023-03-12T20:46:40.080918Z","iopub.status.idle":"2023-03-12T20:46:40.100277Z","shell.execute_reply.started":"2023-03-12T20:46:40.080876Z","shell.execute_reply":"2023-03-12T20:46:40.099368Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_dataset.map(\n    preprocess_function,\n    batch_size=64,\n    batched=True,\n    num_proc=4\n);\n\neval_dataset = eval_dataset.map(\n    preprocess_function,\n    batch_size=64,\n    batched=True,\n    num_proc=4\n);","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:46:40.102804Z","iopub.execute_input":"2023-03-12T20:46:40.103079Z","iopub.status.idle":"2023-03-12T20:47:39.637861Z","shell.execute_reply.started":"2023-03-12T20:46:40.103053Z","shell.execute_reply":"2023-03-12T20:47:39.636615Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"        ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/13 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b67f4609f014c1497754b1834898454"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/13 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78ac2609370b46feaac3d21621178526"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/13 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff002b5381f44ed1b38740b125418ec4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/13 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8e421a6e12f43948a4e48607c229171"}},"metadata":{}},{"name":"stdout","text":"       ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"986c6239d22346cab920c014581f7e58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"229b44db54344905beef1eb34856d039"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90dc80d9666947e7bb825ed2a76ee979"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1c38c8af1654b7b947ef2e31a647d85"}},"metadata":{}}]},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:47:39.639977Z","iopub.execute_input":"2023-03-12T20:47:39.640637Z","iopub.status.idle":"2023-03-12T20:47:39.647883Z","shell.execute_reply.started":"2023-03-12T20:47:39.640592Z","shell.execute_reply":"2023-03-12T20:47:39.646769Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['speech', 'label', 'emotion', 'input_values', 'attention_mask'],\n    num_rows: 3096\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Building classifier","metadata":{}},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Optional, Tuple\nimport torch\nfrom transformers.file_utils import ModelOutput\n\n\n@dataclass\nclass SpeechClassifierOutput(ModelOutput):\n    loss: Optional[torch.FloatTensor] = None\n    logits: torch.FloatTensor = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:47:39.649481Z","iopub.execute_input":"2023-03-12T20:47:39.650158Z","iopub.status.idle":"2023-03-12T20:47:40.458058Z","shell.execute_reply.started":"2023-03-12T20:47:39.650120Z","shell.execute_reply":"2023-03-12T20:47:40.457008Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.models.wav2vec2.modeling_wav2vec2 import (\n    Wav2Vec2PreTrainedModel,\n    Wav2Vec2Model\n)\n\n\nclass Wav2Vec2ClassificationHead(nn.Module):\n    \"\"\"Head for wav2vec classification task.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dropout = nn.Dropout(config.final_dropout)\n        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, features, **kwargs):\n        x = features\n        x = self.dropout(x)\n        x = self.dense(x)\n        x = torch.tanh(x)\n        x = self.dropout(x)\n        x = self.out_proj(x)\n        return x\n\n\nclass Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.pooling_mode = config.pooling_mode\n        self.config = config\n\n        self.wav2vec2 = Wav2Vec2Model(config)\n        self.classifier = Wav2Vec2ClassificationHead(config)\n\n        self.init_weights()\n\n    def freeze_feature_extractor(self):\n        self.wav2vec2.feature_extractor._freeze_parameters()\n\n    def merged_strategy(\n            self,\n            hidden_states,\n            mode=\"mean\"\n    ):\n        if mode == \"mean\":\n            outputs = torch.mean(hidden_states, dim=1)\n        elif mode == \"sum\":\n            outputs = torch.sum(hidden_states, dim=1)\n        elif mode == \"max\":\n            outputs = torch.max(hidden_states, dim=1)[0]\n        else:\n            raise Exception(\n                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n\n        return outputs\n\n    def forward(\n            self,\n            input_values,\n            attention_mask=None,\n            output_attentions=None,\n            output_hidden_states=None,\n            return_dict=None,\n            labels=None,\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        outputs = self.wav2vec2(\n            input_values,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = outputs[0]\n        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n        logits = self.classifier(hidden_states)\n\n        loss = None\n        if labels is not None:\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(logits, labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SpeechClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:47:40.459674Z","iopub.execute_input":"2023-03-12T20:47:40.460033Z","iopub.status.idle":"2023-03-12T20:47:41.734706Z","shell.execute_reply.started":"2023-03-12T20:47:40.459993Z","shell.execute_reply":"2023-03-12T20:47:41.733463Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Padding strategy ","metadata":{}},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Dict, List, Optional, Union\nimport torch\n\nimport transformers\nfrom transformers import Wav2Vec2Processor\n\n\n@dataclass\nclass DataCollatorCTCWithPadding:\n    \"\"\"\n    Data collator that will dynamically pad the inputs received.\n    Args:\n        processor (:class:`~transformers.Wav2Vec2Processor`)\n            The processor used for proccessing the data.\n        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n            among:\n            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n              sequence if provided).\n            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n              maximum acceptable input length for the model if that argument is not provided.\n            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n              different lengths).\n        max_length (:obj:`int`, `optional`):\n            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n        max_length_labels (:obj:`int`, `optional`):\n            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n        pad_to_multiple_of (:obj:`int`, `optional`):\n            If set will pad the sequence to a multiple of the provided value.\n            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n            7.5 (Volta).\n    \"\"\"\n\n    processor: Wav2Vec2Processor\n    padding: Union[bool, str] = True\n    max_length: Optional[int] = None\n    max_length_labels: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    pad_to_multiple_of_labels: Optional[int] = None\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n        label_features = [feature[\"label\"] for feature in features]\n\n        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n\n        batch = self.processor.pad(\n            input_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"pt\",\n        )\n\n        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n\n        return batch","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:47:41.736870Z","iopub.execute_input":"2023-03-12T20:47:41.737240Z","iopub.status.idle":"2023-03-12T20:47:41.750738Z","shell.execute_reply.started":"2023-03-12T20:47:41.737207Z","shell.execute_reply":"2023-03-12T20:47:41.749717Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:47:41.752524Z","iopub.execute_input":"2023-03-12T20:47:41.753274Z","iopub.status.idle":"2023-03-12T20:47:41.767242Z","shell.execute_reply.started":"2023-03-12T20:47:41.753169Z","shell.execute_reply":"2023-03-12T20:47:41.766263Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Evaluation metrics ","metadata":{}},{"cell_type":"code","source":"is_regression = False","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:47:41.769018Z","iopub.execute_input":"2023-03-12T20:47:41.769466Z","iopub.status.idle":"2023-03-12T20:47:41.777650Z","shell.execute_reply.started":"2023-03-12T20:47:41.769374Z","shell.execute_reply":"2023-03-12T20:47:41.776660Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom transformers import EvalPrediction\n\n\ndef compute_metrics(p: EvalPrediction):\n    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n\n    if is_regression:\n        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n    else:\n        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:47:41.778766Z","iopub.execute_input":"2023-03-12T20:47:41.779280Z","iopub.status.idle":"2023-03-12T20:47:41.795831Z","shell.execute_reply.started":"2023-03-12T20:47:41.779242Z","shell.execute_reply":"2023-03-12T20:47:41.794772Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"Building Model","metadata":{}},{"cell_type":"code","source":"model = Wav2Vec2ForSpeechClassification.from_pretrained(\n    model_name_or_path,\n    config=config,\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:47:41.797443Z","iopub.execute_input":"2023-03-12T20:47:41.797849Z","iopub.status.idle":"2023-03-12T20:47:57.370816Z","shell.execute_reply.started":"2023-03-12T20:47:41.797813Z","shell.execute_reply":"2023-03-12T20:47:57.369788Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.26G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20b7ff049a1b43d69bebb0c0dc911dc3"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic were not used when initializing Wav2Vec2ForSpeechClassification: ['lm_head.bias', 'lm_head.weight']\n- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:47:57.372533Z","iopub.execute_input":"2023-03-12T20:47:57.372921Z","iopub.status.idle":"2023-03-12T20:47:57.386557Z","shell.execute_reply.started":"2023-03-12T20:47:57.372883Z","shell.execute_reply":"2023-03-12T20:47:57.385463Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"Wav2Vec2ForSpeechClassification(\n  (wav2vec2): Wav2Vec2Model(\n    (feature_extractor): Wav2Vec2FeatureEncoder(\n      (conv_layers): ModuleList(\n        (0): Wav2Vec2LayerNormConvLayer(\n          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation): GELUActivation()\n        )\n        (1): Wav2Vec2LayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation): GELUActivation()\n        )\n        (2): Wav2Vec2LayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation): GELUActivation()\n        )\n        (3): Wav2Vec2LayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation): GELUActivation()\n        )\n        (4): Wav2Vec2LayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation): GELUActivation()\n        )\n        (5): Wav2Vec2LayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation): GELUActivation()\n        )\n        (6): Wav2Vec2LayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation): GELUActivation()\n        )\n      )\n    )\n    (feature_projection): Wav2Vec2FeatureProjection(\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (projection): Linear(in_features=512, out_features=1024, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): Wav2Vec2EncoderStableLayerNorm(\n      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n        (conv): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n        (padding): Wav2Vec2SamePadLayer()\n        (activation): GELUActivation()\n      )\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (layers): ModuleList(\n        (0): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (6): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (7): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (8): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (9): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (10): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (11): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (12): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (13): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (14): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (15): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (16): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (17): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (18): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (19): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (20): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (21): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (22): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n        (23): Wav2Vec2EncoderLayerStableLayerNorm(\n          (attention): Wav2Vec2Attention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (classifier): Wav2Vec2ClassificationHead(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"model.freeze_feature_extractor()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:47:57.388253Z","iopub.execute_input":"2023-03-12T20:47:57.391200Z","iopub.status.idle":"2023-03-12T20:47:59.201155Z","shell.execute_reply.started":"2023-03-12T20:47:57.391157Z","shell.execute_reply":"2023-03-12T20:47:59.199987Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Building Trainer","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"/content/wav2vec2-xlsr-arabic-speech-emotion-recognition\",\n    # output_dir=\"/content/gdrive/MyDrive/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n    evaluation_strategy=\"epoch\",\n    num_train_epochs=6.0,\n    learning_rate=1e-4,\n    fp16 = True\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:47:59.202358Z","iopub.execute_input":"2023-03-12T20:47:59.202692Z","iopub.status.idle":"2023-03-12T20:47:59.361694Z","shell.execute_reply.started":"2023-03-12T20:47:59.202663Z","shell.execute_reply":"2023-03-12T20:47:59.360638Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    data_collator=data_collator,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=processor.feature_extractor,\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:47:59.367442Z","iopub.execute_input":"2023-03-12T20:47:59.368062Z","iopub.status.idle":"2023-03-12T20:48:06.596364Z","shell.execute_reply.started":"2023-03-12T20:47:59.368029Z","shell.execute_reply":"2023-03-12T20:48:06.595113Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"Using cuda_amp half precision backend\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T20:48:06.597936Z","iopub.execute_input":"2023-03-12T20:48:06.598417Z","iopub.status.idle":"2023-03-12T21:19:25.845470Z","shell.execute_reply.started":"2023-03-12T20:48:06.598374Z","shell.execute_reply":"2023-03-12T21:19:25.844568Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: emotion, speech. If emotion, speech are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 3096\n  Num Epochs = 6\n  Instantaneous batch size per device = 4\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 2\n  Total optimization steps = 1158\n  Number of trainable parameters = 312281219\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.11 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.13.10"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230312_204830-ycpq8b99</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tomoyesad78/huggingface/runs/ycpq8b99' target=\"_blank\">swept-river-4</a></strong> to <a href='https://wandb.ai/tomoyesad78/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tomoyesad78/huggingface' target=\"_blank\">https://wandb.ai/tomoyesad78/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tomoyesad78/huggingface/runs/ycpq8b99' target=\"_blank\">https://wandb.ai/tomoyesad78/huggingface/runs/ycpq8b99</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1158' max='1158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1158/1158 30:08, Epoch 5/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>0.284101</td>\n      <td>0.919897</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.332221</td>\n      <td>0.908269</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.382500</td>\n      <td>0.203857</td>\n      <td>0.944444</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.382500</td>\n      <td>0.173628</td>\n      <td>0.959948</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.382500</td>\n      <td>0.171092</td>\n      <td>0.963824</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.113800</td>\n      <td>0.151354</td>\n      <td>0.963824</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: emotion, speech. If emotion, speech are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 774\n  Batch size = 8\nThe following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: emotion, speech. If emotion, speech are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 774\n  Batch size = 8\nSaving model checkpoint to /content/wav2vec2-xlsr-arabic-speech-emotion-recognition/checkpoint-500\nConfiguration saved in /content/wav2vec2-xlsr-arabic-speech-emotion-recognition/checkpoint-500/config.json\nModel weights saved in /content/wav2vec2-xlsr-arabic-speech-emotion-recognition/checkpoint-500/pytorch_model.bin\nFeature extractor saved in /content/wav2vec2-xlsr-arabic-speech-emotion-recognition/checkpoint-500/preprocessor_config.json\nThe following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: emotion, speech. If emotion, speech are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 774\n  Batch size = 8\nThe following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: emotion, speech. If emotion, speech are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 774\n  Batch size = 8\nThe following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: emotion, speech. If emotion, speech are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 774\n  Batch size = 8\nSaving model checkpoint to /content/wav2vec2-xlsr-arabic-speech-emotion-recognition/checkpoint-1000\nConfiguration saved in /content/wav2vec2-xlsr-arabic-speech-emotion-recognition/checkpoint-1000/config.json\nModel weights saved in /content/wav2vec2-xlsr-arabic-speech-emotion-recognition/checkpoint-1000/pytorch_model.bin\nFeature extractor saved in /content/wav2vec2-xlsr-arabic-speech-emotion-recognition/checkpoint-1000/preprocessor_config.json\nThe following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: emotion, speech. If emotion, speech are not expected by `Wav2Vec2ForSpeechClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 774\n  Batch size = 8\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1158, training_loss=0.22453994948629272, metrics={'train_runtime': 1879.2024, 'train_samples_per_second': 9.885, 'train_steps_per_second': 0.616, 'total_flos': 1.377238465255247e+18, 'train_loss': 0.22453994948629272, 'epoch': 6.0})"},"metadata":{}}]}]}